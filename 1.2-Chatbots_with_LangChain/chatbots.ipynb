{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building a Chatbot**\n",
    "\n",
    "Here, we will see how  to design and implement a LLM powered chatbot. This chatbot will be able to have a conversation and capability to remember previous interactions.\n",
    "\n",
    "Note: In this project, this chatbot will only use the language model to have a conversation. There are several other related concepts that you may be looking for: \n",
    "* **Conversation RAG:** Enable a chatbot experience over an external source of data.\n",
    "* **Agents:** Build a chatbot that can take actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here Basic chatbot is implemented only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000016AD4A8FCD0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000016AD4AB8E50>, model_name='deepseek-r1-distill-llama-70b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"deepseek-r1-distill-llama-70b\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\n\\n</think>\\n\\nHi Rohit! Welcome to the world of GenAI learning! It's exciting that you're diving into this field. How did you get interested in GenAI, and what specific areas or applications are you most curious about? Let me know how I can assist you on your learning journey!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 15, 'total_tokens': 78, 'completion_time': 0.229090909, 'prompt_time': 0.003680159, 'queue_time': 0.258589854, 'total_time': 0.232771068}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_492bd52206', 'finish_reason': 'stop', 'logprobs': None}, id='run-00cdc067-0297-4e43-a748-0efa13d232c6-0', usage_metadata={'input_tokens': 15, 'output_tokens': 63, 'total_tokens': 78})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "humanMsg=HumanMessage(content=\"Hi, Name is Rohit and I am learning GenAI\")\n",
    "model.invoke([humanMsg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying basic approach to see that whether the model is remembering the previous response or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user just asked, \"Hey, what is my name and what I am doing?\" Let me check the conversation history. In the previous message, the user introduced himself as Rohit and mentioned he\\'s learning GenAI. So, I should use that information to respond.\\n\\nI need to make sure I address him by name to keep it personal. Also, I should acknowledge his learning journey in GenAI. Maybe I can offer further assistance to show I\\'m here to help.\\n\\nHmm, should I add some encouragement? That might make the response more friendly and supportive. Let me phrase it in a way that invites him to ask more questions if he needs help.\\n</think>\\n\\nYour name is Rohit, and you\\'re learning about GenAI! How can I assist you further on your learning journey? 😊', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 88, 'total_tokens': 255, 'completion_time': 0.607272727, 'prompt_time': 0.00960383, 'queue_time': 0.251616223, 'total_time': 0.616876557}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_3de5b607fa', 'finish_reason': 'stop', 'logprobs': None}, id='run-194629a2-5cdb-4fa7-a553-6788354d297a-0', usage_metadata={'input_tokens': 88, 'output_tokens': 167, 'total_tokens': 255})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke([\n",
    "  humanMsg,\n",
    "  AIMessage(content=\"Hi Rohit! Welcome to the world of GenAI learning! It's exciting that you're diving into this field. How did you get interested in GenAI, and what specific areas or applications are you most curious about? Let me know how I can assist you on your learning journey!\"),\n",
    "  HumanMessage(content=\"Hey, what is my name and what I am doing?\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Message History** \\\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and stores them in some datastore. \\\n",
    "Future interactions will then load those messages and pass them into the chain as part of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "  if session_id not in store:\n",
    "    store[session_id]=ChatMessageHistory()\n",
    "  return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nHello, Rohit! It's great to hear that you're learning about General Artificial Intelligence (GenAI). This field is rapidly evolving and offers a lot of exciting opportunities. Here are some key points to keep in mind as you progress in your learning journey:\\n\\n### 1. **Understanding GenAI:**\\n   - **Definition**: GenAI refers to advanced AI systems that can perform a wide range of tasks, similar to human intelligence. These systems are designed to be versatile and adaptable across different domains.\\n   - **Applications**: GenAI can be applied in areas like natural language processing, computer vision, robotics, and more.\\n\\n### 2. **Starting with the Basics:**\\n   - **Machine Learning (ML)**: Begin by understanding the fundamentals of machine learning, including supervised, unsupervised, and reinforcement learning.\\n   - **Deep Learning (DL)**: Dive into deep learning concepts, such as neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).\\n\\n### 3. **Hands-On Practice:**\\n   - **Python and Libraries**: Python is a popular language for AI development. Familiarize yourself with libraries like TensorFlow, Keras, and PyTorch.\\n   - **Projects**: Start with simple projects like image classification, text classification, or chatbots to apply your knowledge.\\n\\n### 4. **Stay Updated:**\\n   - **Research Papers**: Follow recent research papers and publications to stay updated with the latest advancements in GenAI.\\n   - **Blogs and Podcasts**: Subscribe to AI-related blogs and podcasts to gain insights from experts in the field.\\n\\n### 5. **Ethical Considerations:**\\n   - **Bias and Fairness**: Understand the importance of bias mitigation and fairness in AI systems.\\n   - **Responsible AI**: Learn about the ethical implications of AI and the importance of responsible AI development.\\n\\n### 6. **Community Engagement:**\\n   - **Forums and Groups**: Join AI-related forums, groups, and communities to connect with other learners and professionals.\\n   - **Workshops and Conferences**: Attend workshops and conferences to network and learn from experts.\\n\\n### 7. **Continuous Learning:**\\n   - **Adaptability**: Be prepared to continuously learn and adapt as the field of GenAI evolves.\\n   - **Problem-Solving**: Focus on developing strong problem-solving skills, as they are crucial in AI development.\\n\\n### 8. **Hardware Considerations:**\\n   - **Computational Resources**: Depending on your projects, you may need access to powerful computational resources. Consider using cloud services like Google Colab, AWS, or Azure for your projects.\\n\\n### 9. **Career Opportunities:**\\n   - **Job Market**: Explore the various career opportunities available in the field of GenAI, such as AI engineer, data scientist, machine learning engineer, etc.\\n   - **Networking**: Build a strong network of professionals in the AI field to explore opportunities and gain insights.\\n\\n### 10. **Stay Motivated:**\\n   - **Set Goals**: Define clear goals for your learning journey to stay motivated.\\n   - **Celebrate Milestones**: Acknowledge and celebrate your achievements along the way to keep your morale high.\\n\\n### Conclusion:\\nLearning GenAI is a rewarding journey that requires dedication, continuous learning, and a passion for problem-solving. By starting with the basics, engaging in hands-on practice, and staying updated with the latest trends and ethical considerations, you can make significant progress in this field. Good luck, Rohit!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 714, 'prompt_tokens': 939, 'total_tokens': 1653, 'completion_time': 2.596363636, 'prompt_time': 0.068289914, 'queue_time': 0.307420893, 'total_time': 2.66465355}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_3de5b607fa', 'finish_reason': 'stop', 'logprobs': None}, id='run-928e7833-6de3-429f-8529-22143aa65566-0', usage_metadata={'input_tokens': 939, 'output_tokens': 714, 'total_tokens': 1653})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke([\n",
    "  humanMsg,\n",
    "],config=config)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nHello, Rohit! It's great to hear that you're learning about General Artificial Intelligence (GenAI). This field is rapidly evolving and offers a lot of exciting opportunities. Here are some key points to keep in mind as you progress in your learning journey:\\n\\n### 1. **Understanding GenAI:**\\n   - **Definition**: GenAI refers to advanced AI systems that can perform a wide range of tasks, similar to human intelligence. These systems are designed to be versatile and adaptable across different domains.\\n   - **Applications**: GenAI can be applied in areas like natural language processing, computer vision, robotics, and more.\\n\\n### 2. **Starting with the Basics:**\\n   - **Machine Learning (ML)**: Begin by understanding the fundamentals of machine learning, including supervised, unsupervised, and reinforcement learning.\\n   - **Deep Learning (DL)**: Dive into deep learning concepts, such as neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).\\n\\n### 3. **Hands-On Practice:**\\n   - **Python and Libraries**: Python is a popular language for AI development. Familiarize yourself with libraries like TensorFlow, Keras, and PyTorch.\\n   - **Projects**: Start with simple projects like image classification, text classification, or chatbots to apply your knowledge.\\n\\n### 4. **Stay Updated:**\\n   - **Research Papers**: Follow recent research papers and publications to stay updated with the latest advancements in GenAI.\\n   - **Blogs and Podcasts**: Subscribe to AI-related blogs and podcasts to gain insights from experts in the field.\\n\\n### 5. **Ethical Considerations:**\\n   - **Bias and Fairness**: Understand the importance of bias mitigation and fairness in AI systems.\\n   - **Responsible AI**: Learn about the ethical implications of AI and the importance of responsible AI development.\\n\\n### 6. **Community Engagement:**\\n   - **Forums and Groups**: Join AI-related forums, groups, and communities to connect with other learners and professionals.\\n   - **Workshops and Conferences**: Attend workshops and conferences to network and learn from experts.\\n\\n### 7. **Continuous Learning:**\\n   - **Adaptability**: Be prepared to continuously learn and adapt as the field of GenAI evolves.\\n   - **Problem-Solving**: Focus on developing strong problem-solving skills, as they are crucial in AI development.\\n\\n### 8. **Hardware Considerations:**\\n   - **Computational Resources**: Depending on your projects, you may need access to powerful computational resources. Consider using cloud services like Google Colab, AWS, or Azure for your projects.\\n\\n### 9. **Career Opportunities:**\\n   - **Job Market**: Explore the various career opportunities available in the field of GenAI, such as AI engineer, data scientist, machine learning engineer, etc.\\n   - **Networking**: Build a strong network of professionals in the AI field to explore opportunities and gain insights.\\n\\n### 10. **Stay Motivated:**\\n   - **Set Goals**: Define clear goals for your learning journey to stay motivated.\\n   - **Celebrate Milestones**: Acknowledge and celebrate your achievements along the way to keep your morale high.\\n\\n### Conclusion:\\nLearning GenAI is a rewarding journey that requires dedication, continuous learning, and a passion for problem-solving. By starting with the basics, engaging in hands-on practice, and staying updated with the latest trends and ethical considerations, you can make significant progress in this field. Good luck, Rohit!\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\nYour name is Rohit.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 1660, 'total_tokens': 1669, 'completion_time': 0.032727273, 'prompt_time': 0.143628351, 'queue_time': 0.23945379500000002, 'total_time': 0.176355624}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_3de5b607fa', 'finish_reason': 'stop', 'logprobs': None}, id='run-574f8ca8-cd0a-4f90-8813-846b96fd5cca-0', usage_metadata={'input_tokens': 1660, 'output_tokens': 9, 'total_tokens': 1669})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([\n",
    "  HumanMessage(content=\"What is my name?\")\n",
    "],config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Change the config->session_id changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nYour name is Rohit.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response=with_message_history.invoke(\n",
    "  [HumanMessage(content=\"What is my name?\")],\n",
    "  config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above scenario, session id is not change that's why model remembers the interactions.\n",
    "\n",
    "Let's change the session id now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\n\\n</think>\\n\\nHi! I'm DeepSeek-R1, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "  [HumanMessage(content=\"What is my name?\")],\n",
    "  config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, by changing the session_id the model do not remember the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nAlright, the user just told me their name is Rahul. I should respond in a friendly and welcoming manner. Since they shared their name, I can use it to make the interaction more personal. I want to make sure they feel comfortable asking for help, so I'll keep the tone positive and open. Maybe I can also offer further assistance to encourage them to continue the conversation.\\n</think>\\n\\nHey Rahul! How can I assist you today?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "  [HumanMessage(content=\"Hey, my name is Rahul\")],\n",
    "  config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nYou told me your name is Rahul. Let me know how I can assist you!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "  [HumanMessage(content=\"What is my name?\")],\n",
    "  config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "**Prompt Templates**\\\n",
    "Prompt Templates help to turn raw user info into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\",\"You are a helpful assistant. Answer the questions to be best of your ability\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "  ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I just saw this message where someone named Rohit introduced himself. I need to figure out how to respond appropriately. Since I\\'m supposed to be a helpful assistant, I should make sure my reply is friendly and open. \\n\\nFirst, I should acknowledge his greeting. Maybe start with a hello or hi. Then, I can ask how I can assist him today. It\\'s important to keep it simple and welcoming so he feels comfortable asking for help.\\n\\nI should avoid making it too long; just a brief response. Also, I should use a friendly tone without any formal language. That way, it feels more approachable.\\n\\nWait, in the example response, they used \"Hello Rohit! How can I assist you today?\" That\\'s pretty straightforward. Maybe I can use that as a model but put it in my own words. Or maybe just stick with that since it\\'s concise and effective.\\n\\nI don\\'t want to overcomplicate it. The main goal is to respond in a way that invites him to ask for help. So, keeping it short and friendly is the way to go.\\n</think>\\n\\nHello Rohit! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 239, 'prompt_tokens': 25, 'total_tokens': 264, 'completion_time': 0.869090909, 'prompt_time': 0.003994535, 'queue_time': 0.23675849100000002, 'total_time': 0.873085444}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_3de5b607fa', 'finish_reason': 'stop', 'logprobs': None}, id='run-6d35c7ae-4abd-4c63-acc7-796f251d379c-0', usage_metadata={'input_tokens': 25, 'output_tokens': 239, 'total_tokens': 264})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "  {\"messages\":[HumanMessage(content=\"Hi my name is Rohit\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "config3={\"configurable\":{\"session_id\":\"chat3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "  [HumanMessage(content=\"Hi, my name is Rohit\")],\n",
    "  config=config3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nHi Rohit! Welcome. How can I assist you today?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some complexity in prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\",\"You are a helpful assistant. Answer the questions to be best of your ability in {language}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "  ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke(\n",
    "  {\n",
    "    \"messages\":[HumanMessage(content=\"Hi my name is Rohit\")],\n",
    "    \"language\":\"Hindi\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user greeted me with \"Hi my name is Rohit.\" I should respond in a friendly and welcoming manner. Since they mentioned their name, I\\'ll include that to make it personal. I should also offer my assistance to show I\\'m ready to help with whatever they need. Keeping it simple and warm should make them feel comfortable to ask questions.\\n</think>\\n\\nनमस्ते Rohit! मैं आपकी मदद के लिए यहाँ हूँ। आप क्या जानना चाहते हैं या मैं आपकी कैसे सहायता कर सकता हूँ?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wrap this more complicated chain in a Message History class. Now because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "  chain,\n",
    "  get_session_history,\n",
    "  input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nAlright, the user greeted me in Hindi. I should respond warmly in the same language.\\n\\nThey introduced themselves as Rohit Gupta. I'll acknowledge that and offer my help.\\n\\nI need to keep it friendly and open-ended so they feel comfortable asking for assistance.\\n</think>\\n\\nनमस्ते रोहित गुप्ता जी! मैं आपकी मदद के लिए तैयार हूँ। कृपया अपना प्रश्न बताएं या बताएं कि मैं आपकी कैसे सहायता कर सकता हूँ।\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config4={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "  {\n",
    "    \"messages\":[HumanMessage(content=\"Hi I am Rohit Gupta\")],\n",
    "    \"language\":\"Hindi\"\n",
    "  },\n",
    "  config=config4\n",
    "\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, the user just said \"Hey, what is my name?\" in English. \\n\\nLooking back at the history, the user introduced himself as Rohit Gupta earlier.\\n\\nI should respond in Hindi as per the initial instruction.\\n\\nI need to confirm his name politely.\\n\\nSo, I\\'ll say his name is Rohit Gupta and offer further assistance.\\n</think>\\n\\nआपका नाम रोहित गुप्ता है, सही है ना? कृपया बताएं कि मैं आपकी और कैसे मदद कर सकता हूँ!'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "  {\n",
    "    \"messages\":[HumanMessage(content=\"Hey, what is my name?\")],\n",
    "    \"language\":\"Hindi\"\n",
    "  },\n",
    "  config=config4\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Managing the Conversation History**\n",
    "\n",
    "If the chatbot conversation history is unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM.\\\n",
    "Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "\n",
    "`trim_messages` : helper to redunce the number of messages we are sending to the model. \\\n",
    "The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is 2+2?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='No problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Have fun..', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='woah!!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer=trim_messages(\n",
    "  max_tokens=50,\n",
    "  strategy=\"last\",\n",
    "  token_counter=model,\n",
    "  include_system=True,\n",
    "  allow_partial=False,\n",
    "  start_on=\"human\"\n",
    ")\n",
    "messages=[\n",
    "  SystemMessage(content=\"You are a good assistant\"),\n",
    "  HumanMessage(content=\"Hi, I am Rohit\"),\n",
    "  AIMessage(content=\"Hi!\"),\n",
    "  HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "  AIMessage(content=\"nice\"),\n",
    "  HumanMessage(content=\"what is 2+2?\"),\n",
    "  AIMessage(content=\"4\"),\n",
    "  HumanMessage(content=\"thanks\"),\n",
    "  AIMessage(content=\"No problem!\"),\n",
    "  HumanMessage(content=\"Have fun..\"),\n",
    "  AIMessage(content='woah!!')\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a chain with trimmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, the user just asked, \"What ice cream do I like.\" Hmm, I don\\'t have access to personal data, so I can\\'t know their preferences. I should let them know that. Maybe I can offer some popular options to help them decide. Let me list a few common flavors like vanilla, chocolate, strawberry, and maybe some more unique ones like cookies and cream or mint chocolate chip. That way, they can think about which one they might like. I should keep it friendly and open-ended, encouraging them to share if they want. Yeah, that sounds good.\\n</think>\\n\\nI don\\'t have access to personal data, so I don\\'t know your preferences, but I can suggest some popular ice cream flavors if you\\'d like! For example, vanilla, chocolate, strawberry, cookies and cream, or mint chocolate chip. Let me know if you\\'d like more suggestions or help with anything else! 😊'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "  RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "  | prompt\n",
    "  | model\n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "  {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What icecream do I like\")],\n",
    "    \"language\":\"English\"\n",
    "  }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above, the trimmer removed the message related to ice cream bcz of maximum tokens, That is why the model is unable to repond correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, the user is asking, \"What math problem did I ask?\" Let me check the history. \\n\\nLooking back, the user previously asked, \"what is 2+2?\" and I responded with \"4\". \\n\\nSo, they\\'re now inquiring about the specific math problem they had asked before. \\n\\nI should confirm the problem and the answer to make sure they\\'re on the same page. \\n\\nMaybe they\\'re trying to recall or verify the question for their records or further use. \\n\\nI\\'ll restate the problem and the solution clearly to provide the information they need.\\n</think>\\n\\nYou asked, \"What is 2+2?\" and the answer was **4**. 😊'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\n",
    "  {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What math problem did I ask?\")],\n",
    "    \"language\":\"English\"\n",
    "  }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap up in Message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "  chain,\n",
    "  get_session_history,\n",
    "  input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, the user asked \"what is my name?\" in their last message. I need to figure out how to respond appropriately. \\n\\nLooking at the conversation history, I see that I\\'ve been helpful so far. They asked for 2+2 and I gave the correct answer, then they thanked me, and I responded politely. Now they\\'re asking for their name.\\n\\nI don\\'t have access to personal information about the user unless they\\'ve shared it before. Since this is the start of our interaction, I can assume I don\\'t know their name. \\n\\nI should let them know that I can\\'t access personal information. It\\'s important to respect their privacy and be transparent about my limitations. \\n\\nMaybe I can offer to help with something else to keep the conversation going. That way, they know I\\'m still here to assist them in other ways.\\n</think>\\n\\nI don\\'t have access to personal information, including your name. If you\\'d like, you can share it with me, and I\\'ll do my best to assist you! 😊'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    " { \n",
    "   \"messages\":messages+[HumanMessage(content=\"what is my name?\")],\n",
    "   \"language\":\"English\"\n",
    " },\n",
    "  config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user just asked, \"what math problem did I ask?\" Hmm, let me think. Looking back at the conversation history, the user started by asking \"what is 2+2?\" and I replied with \"4\". Then the user said \"thanks\" and I responded with \"No problem!\" After that, the user wrote \"Have fun..\" and I reacted with \"woah!!\". Now, the user is asking about the math problem they asked. \\n\\nWait, so the user might be a bit confused or maybe they\\'re testing if I can recall our previous conversation. I should check the history to make sure. Yeah, the math problem was indeed 2+2. Maybe the user is verifying if I remember correctly or if they want to confirm something. \\n\\nI should respond clearly, stating that they asked for 2+2 and that the answer was 4. It\\'s important to be precise and helpful. I don\\'t see any other math problems in our conversation, so it\\'s safe to assume that\\'s the one they\\'re referring to. I should also keep the tone friendly and open, inviting them to ask more if they need further assistance. \\n\\nI wonder if the user is trying to test my memory or if they genuinely forgot. Either way, providing the correct information is key. I should make sure my response is concise and accurate to avoid any confusion. Also, using a friendly emoji like 😊 can make the interaction more pleasant. \\n\\nAlright, so putting it all together, I\\'ll confirm the math problem and the answer, and let them know I\\'m here for more help if needed.\\n</think>\\n\\nYou asked, \"What is 2+2?\" and the answer was **4**. 😊 Let me know if you need help with anything else!'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    " { \n",
    "   \"messages\":messages+[HumanMessage(content=\"what math problem did I ask?\")],\n",
    "   \"language\":\"English\"\n",
    " },\n",
    "  config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
