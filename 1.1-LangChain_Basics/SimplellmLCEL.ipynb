{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a Simple LLM Application with LCEL**\n",
    "\n",
    "Here, we will see how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM applications-it is a single LLM call with prompting. \\\n",
    "With this one should have a high level overview of: \n",
    "* Using Large language models\n",
    "* Using PromptTemplates and OutputParsers\n",
    "* Using LangChain Expression Language (LCEL) to chain components together\n",
    "* Debugging and tracing your application using LangSmith\n",
    "* Deploying your application with LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI API Key and Open-Source Models->Llama3, Gemma2, mistral->Groq API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Groq & the Language Processing Unit** \\\n",
    "Groq builds fast AI inference. The Groq LPU™, AI Inference Technology, delivers exceptional compute speed, affordability, and energy efficiency at scale. \n",
    "\n",
    "Groq solutions are based on the Language Processing Unit (LPU), a new\n",
    "category of processor. Groq is the creator of the LPU and built it from the ground up to meet the unique characteristics and needs of AI. \n",
    "\n",
    "LPUs run Large Language Models (LLMs) at substantially faster speeds and, on an architectural level, up to 10x better energy efficiency compared to GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F51AFF2200>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F51AFF35B0>, model_name='deepseek-r1-distill-llama-70b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model=ChatGroq(model=\"deepseek-r1-distill-llama-70b\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I need to translate \"Hello, How are you?\" into French. Let me start by breaking down the sentence. \"Hello\" is a common greeting, and in French, the direct equivalent is \"Bonjour.\" That\\'s straightforward.\\n\\nNext, \"How are you?\" is a question asking about someone\\'s well-being. In French, the common way to ask this is \"Comment allez-vous?\" which is formal, or \"Comment ça va?\" informally. Since the original sentence doesn\\'t specify formality, I\\'ll go with the formal version to keep it general.\\n\\nPutting it together, \"Hello, How are you?\" becomes \"Bonjour, comment allez-vous?\" I should also consider punctuation. The original has a comma after \"Hello,\" so I\\'ll include a comma after \"Bonjour\" as well.\\n\\nWait, maybe I should check if there\\'s a more natural way to structure it in French. Sometimes greetings and questions are combined differently. But I think \"Bonjour, comment allez-vous?\" is the standard and most polite way to say it. I don\\'t see any mistakes here, so I think that\\'s the correct translation.\\n</think>\\n\\nBonjour, comment allez-vous ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 243, 'prompt_tokens': 17, 'total_tokens': 260, 'completion_time': 0.883636364, 'prompt_time': 0.003837087, 'queue_time': 0.233879799, 'total_time': 0.887473451}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_9397364173', 'finish_reason': 'stop', 'logprobs': None}, id='run-f24e73d4-5c06-4c24-86fe-b3960a6193bb-0', usage_metadata={'input_tokens': 17, 'output_tokens': 243, 'total_tokens': 260})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "messages=[\n",
    "  SystemMessage(content=\"Translate the following from English to French\"),\n",
    "  HumanMessage(content=\"Hello, How are you?\")\n",
    "]\n",
    "\n",
    "response=model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to translate \"Hello, How are you?\" into French. Let me start by breaking down the sentence. \"Hello\" is a common greeting, and in French, the direct equivalent is \"Bonjour.\" That\\'s straightforward.\\n\\nNext, \"How are you?\" is a question asking about someone\\'s well-being. In French, the common way to ask this is \"Comment allez-vous?\" which is formal, or \"Comment ça va?\" informally. Since the original sentence doesn\\'t specify formality, I\\'ll go with the formal version to keep it general.\\n\\nPutting it together, \"Hello, How are you?\" becomes \"Bonjour, comment allez-vous?\" I should also consider punctuation. The original has a comma after \"Hello,\" so I\\'ll include a comma after \"Bonjour\" as well.\\n\\nWait, maybe I should check if there\\'s a more natural way to structure it in French. Sometimes greetings and questions are combined differently. But I think \"Bonjour, comment allez-vous?\" is the standard and most polite way to say it. I don\\'t see any mistakes here, so I think that\\'s the correct translation.\\n</think>\\n\\nBonjour, comment allez-vous ?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser=StrOutputParser()\n",
    "parser.invoke(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Using LCEL we can chain the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, the user wants to translate \"Hello, How are you?\" into French. I know the basic greetings in French. \"Hello\" is \"Bonjour.\" Now, for \"How are you?\" the common translation is \"Comment ça va?\" That should cover it. Let me put it together: \"Bonjour, comment ça va?\" That sounds natural and polite. I think that\\'s the best way to translate it.\\n</think>\\n\\nBonjour, comment ça va ?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=model|parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Using PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "generic_template=\"Translate the following into {language}\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\",generic_template),\n",
    "    (\"user\",\"{text}\")\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=prompt.invoke({\"language\":\"French\",\"text\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into French', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chaining components with LCEL** \\\n",
    "prompt->model->parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nBonjour'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|model|parser\n",
    "chain.invoke({\"language\":\"French\",\"text\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
