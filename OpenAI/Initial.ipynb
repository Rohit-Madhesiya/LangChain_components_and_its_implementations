{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Started with Langchain and OpenAI**\n",
    "\n",
    "In the quickstart, we will see how to: \n",
    "* Get setup with LangChain, LangSmith, LangServe\n",
    "* Use the most basic and common components: prompt templates, models, and output parsers \n",
    "* Build a simple application with \n",
    "* Trace the application with LangSmith\n",
    "* Serve the application with LangServe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n",
    "# LangSmith Tracking\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001F7AE5C3220> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F7AE6412A0> root_client=<openai.OpenAI object at 0x000001F7AE5C1450> root_async_client=<openai.AsyncOpenAI object at 0x000001F7AE5C3160> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model='gpt-4o')\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query and Response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Generative AI, often abbreviated as GenAI, refers to a category of artificial intelligence technology that is designed to generate new content. This can include text, images, audio, video, and more. Unlike traditional AI, which is often used to analyze or recognize patterns in existing data, generative AI models create new data that mimic the patterns or styles of their training data. Some popular examples of generative AI include OpenAI's GPT (Generative Pre-trained Transformer) models for text generation, Google's DeepDream for image creation, and generative adversarial networks (GANs) which have been widely used for generating realistic images. These systems are capable of producing creative and novel outputs and are finding applications in fields ranging from creative industries to advanced scientific research.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 12, 'total_tokens': 164, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5e115050d0', 'finish_reason': 'stop', 'logprobs': None}, id='run-059fea7c-8fa4-4e7d-8b60-310de8dd9427-0', usage_metadata={'input_tokens': 12, 'output_tokens': 152, 'total_tokens': 164, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=llm.invoke(\"What is GenAI?\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI, often abbreviated as GenAI, refers to a category of artificial intelligence technology that is designed to generate new content. This can include text, images, audio, video, and more. Unlike traditional AI, which is often used to analyze or recognize patterns in existing data, generative AI models create new data that mimic the patterns or styles of their training data. Some popular examples of generative AI include OpenAI's GPT (Generative Pre-trained Transformer) models for text generation, Google's DeepDream for image creation, and generative adversarial networks (GANs) which have been widely used for generating realistic images. These systems are capable of producing creative and novel outputs and are finding applications in fields ranging from creative industries to advanced scientific research.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPrompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\",\"You are an expert AI Engineer. Provide me answers based on the question\"),\n",
    "    (\"user\",\"{user_input}\")\n",
    "  ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"LangSmith is a platform developed by the team behind LangChain, designed to streamline the process of developing, testing, and evaluating applications that utilize large language models (LLMs). It offers a suite of features that help developers manage the complexity of working with LLMs and improve the performance and reliability of their applications.\\n\\nKey features of LangSmith include:\\n\\n1. **Monitoring**: LangSmith provides tools to monitor the performance and behavior of language models in real-time. This allows developers to track model outputs, identify issues, and gather insights into how well their applications are functioning.\\n\\n2. **Testing and Evaluation**: It offers capabilities to rigorously test and evaluate models, enabling developers to fine-tune and optimize them for better accuracy and reliability. This includes comparing different models and configurations.\\n\\n3. **Debugging**: LangSmith aids in debugging applications by simplifying the process of diagnosing problems with model outputs and offering solutions or improvements.\\n\\n4. **Integration with LangChain**: Since LangSmith is developed by the same team as LangChain, it is designed to seamlessly integrate with LangChain's tools and libraries, providing a cohesive ecosystem for building and deploying LLM-based applications.\\n\\n5. **User Interface**: The platform typically includes a user-friendly interface that allows for easy navigation and manipulation of its features, making it accessible to both experienced developers and those new to working with LLMs.\\n\\nBy combining these features, LangSmith aims to reduce the complexity and effort involved in developing applications that rely on large language models, ultimately leading to better performance and user experience.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 313, 'prompt_tokens': 33, 'total_tokens': 346, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None} id='run-a1f132ef-1d6d-48ac-87cb-05f1ef2dc95c-0' usage_metadata={'input_tokens': 33, 'output_tokens': 313, 'total_tokens': 346, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm\n",
    "res=chain.invoke({\"user_input\":\"Can you tell me about LangSmith?\"})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stroutput Parser** is the default output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"LangSmith is a platform developed by the team behind LangChain, designed to streamline the process of developing, testing, and evaluating applications that utilize large language models (LLMs). It offers a suite of features that help developers manage the complexity of working with LLMs and improve the performance and reliability of their applications.\\n\\nKey features of LangSmith include:\\n\\n1. **Monitoring**: LangSmith provides tools to monitor the performance and behavior of language models in real-time. This allows developers to track model outputs, identify issues, and gather insights into how well their applications are functioning.\\n\\n2. **Testing and Evaluation**: It offers capabilities to rigorously test and evaluate models, enabling developers to fine-tune and optimize them for better accuracy and reliability. This includes comparing different models and configurations.\\n\\n3. **Debugging**: LangSmith aids in debugging applications by simplifying the process of diagnosing problems with model outputs and offering solutions or improvements.\\n\\n4. **Integration with LangChain**: Since LangSmith is developed by the same team as LangChain, it is designed to seamlessly integrate with LangChain's tools and libraries, providing a cohesive ecosystem for building and deploying LLM-based applications.\\n\\n5. **User Interface**: The platform typically includes a user-friendly interface that allows for easy navigation and manipulation of its features, making it accessible to both experienced developers and those new to working with LLMs.\\n\\nBy combining these features, LangSmith aims to reduce the complexity and effort involved in developing applications that rely on large language models, ultimately leading to better performance and user experience.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 313, 'prompt_tokens': 33, 'total_tokens': 346, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_523b9b6e5f', 'finish_reason': 'stop', 'logprobs': None} id='run-a1f132ef-1d6d-48ac-87cb-05f1ef2dc95c-0' usage_metadata={'input_tokens': 33, 'output_tokens': 313, 'total_tokens': 346, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"user_input\":\"Can you tell me about langchain?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
