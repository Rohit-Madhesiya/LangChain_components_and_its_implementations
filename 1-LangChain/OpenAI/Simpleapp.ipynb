{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Generative AI App using LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Ingestion->From the website, we need to scrape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1824db532b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nManage prompts programmatically | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptUse custom TLS certificatesManage prompts programmaticallyManaging Prompt SettingsPrompt TagsOpen a prompt from a traceTesting over a datasetLangChain HubPrompt CanvasConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringHow-to GuidesManage prompts programmaticallyOn this pageManage prompts programmatically\\nYou can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.\\nnotePreviously this functionality lived in the langchainhub package which is now deprecated.\\nAll functionality going forward will live in the langsmith package.\\nInstall packages\\u200b\\nIn Python, you can directly use the LangSmith SDK (recommended, full functionality) or you can use through the LangChain package (limited to pushing and pulling prompts).\\nIn TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.\\nPythonLangChain (Python)TypeScriptpip install -U langsmith # version >= 0.1.99pip install -U langchain langsmith # langsmith version >= 0.1.99 and langchain >= 0.2.13yarn add langsmith langchain // langsmith version >= 0.1.99 and langchain version >= 0.2.14\\nConfigure environment variables\\u200b\\nIf you already have LANGSMITH_API_KEY set to your current workspace\\'s api key from LangSmith, you can skip this step.\\nOtherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith.\\nSet your environment variable.\\nexport LANGSMITH_API_KEY=\"lsv2_...\"\\nTerminologyWhat we refer to as \"prompts\" used to be called \"repos\", so any references to \"repo\" in the code are referring to a prompt.\\nPush a prompt\\u200b\\nTo create a new prompt or update an existing prompt, you can use the push prompt method.\\nPythonLangChain (Python)TypeScriptfrom langsmith import Clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = client.push_prompt(\"joke-generator\", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = prompts.push(\"joke-generator\", prompt)# url is a link to the prompt in the UIprint(url)import * as hub from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const url = hub.push(\"joke-generator\", {object: prompt,});// url is a link to the prompt in the UIconsole.log(url);\\nYou can also push a prompt as a RunnableSequence of a prompt and a model.\\nThis is useful for storing the model configuration you want to use with this prompt.\\nThe provider must be supported by the LangSmith playground. (see settings here: Supported Providers)\\nPythonLangChain (Python)TypeScriptfrom langsmith import Clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelclient.push_prompt(\"joke-generator-with-model\", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelurl = prompts.push(\"joke-generator-with-model\", chain)# url is a link to the prompt in the UIprint(url)import * as hub from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from \"@langchain/openai\";const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const chain = prompt.pipe(model);await hub.push(\"joke-generator-with-model\", {object: chain});\\nPull a prompt\\u200b\\nTo pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate.\\nTo pull a private prompt you do not need to specify the owner handle (though you can, if you have one set).\\nTo pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt\\'s author.\\nPythonLangChain (Python)TypeScriptfrom langsmith import Clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})import * as hub from \"langchain/hub\";import { ChatOpenAI } from \"@langchain/openai\";const prompt = await hub.pull(\"joke-generator\");const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const chain = prompt.pipe(model);await chain.invoke({\"topic\": \"cats\"});\\nSimilar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.\\nJust specify include_model when pulling the prompt.\\nIf the stored prompt includes a model, it will be returned as a RunnableSequence.\\nMake sure you have the proper environment variables set for the model you are using.\\nPythonLangChain (Python)TypeScriptfrom langsmith import Clientclient = Client()chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})from langchain import hub as promptschain = prompts.pull(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})import * as hub from \"langchain/hub\";import { Runnable } from \"@langchain/core/runnables\";const chain = await hub.pull<Runnable>(\"joke-generator-with-model\", { includeModel: true });await chain.invoke({\"topic\": \"cats\"});\\nWhen pulling a prompt, you can also specify a specific commit hash or prompt tag to pull a specific version of the prompt.\\nPythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\")const prompt = await hub.pull(\"joke-generator:12344e88\")\\nTo pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt\\'s author.\\nPythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")const prompt = await hub.pull(\"efriis/my-first-prompt\")\\nUse a prompt without LangChain\\u200b\\nIf you want to store your prompts in LangSmith but use them directly with a model provider\\'s API, you can use our conversion methods.\\nThese convert your prompt into the payload required for the OpenAI or Anthropic API.\\nThese conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency\\nin addition to your official SDK of choice. Here are some examples:\\nOpenAI\\u200b\\nPythonTypeScriptpip install -U langchain_openaiyarn add @langchain/openai @langchain/core // @langchain/openai version >= 0.3.2\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.client import Client, convert_prompt_to_openai_format# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai_format(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)import * as hub from \"langchain/hub\";import { convertPromptToOpenAI } from \"@langchain/openai\";import OpenAI from \"openai\";const prompt = await hub.pull(\"jacob/joke-generator\");const formattedPrompt = await prompt.invoke({topic: \"cats\",});const { messages } = convertPromptToOpenAI(formattedPrompt);const openAIClient = new OpenAI();const openAIResponse = await openAIClient.chat.completions.create({model: \"gpt-4o-mini\",messages,});\\nAnthropic\\u200b\\nPythonTypeScriptpip install -U langchain_anthropicyarn add @langchain/anthropic @langchain/core // @langchain/anthropic version >= 0.3.3\\nPythonTypeScriptfrom anthropic import Anthropicfrom langsmith.client import Client, convert_prompt_to_anthropic_format# langsmith clientclient = Client()# anthropic clientanthropic_client = Anthropic()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})anthropic_payload = convert_prompt_to_anthropic_format(prompt_value)anthropic_response = anthropic_client.messages.create(**anthropic_payload)import * as hub from \"langchain/hub\";import { convertPromptToAnthropic } from \"@langchain/anthropic\";import Anthropic from \"@anthropic-ai/sdk\";const prompt = await hub.pull(\"jacob/joke-generator\");const formattedPrompt = await prompt.invoke({topic: \"cats\",});const { messages, system } = convertPromptToAnthropic(formattedPrompt);const anthropicClient = new Anthropic();const anthropicResponse = await anthropicClient.messages.create({model: \"claude-3-haiku-20240307\",system,messages,max_tokens: 1024,stream: false,});\\nList, delete, and like prompts\\u200b\\nYou can also list, delete, and like/unlike prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.\\nSee the LangSmith SDK client for extensive documentation on these methods.\\nPythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include \"joke\"prompts = client.list_prompts(query=\"joke\", is_public=False)# Delete a promptclient.delete_prompt(\"joke-generator\")# Like a promptclient.like_prompt(\"efriis/my-first-prompt\")# Unlike a promptclient.unlike_prompt(\"efriis/my-first-prompt\")// List all prompts in my workspaceimport Client from \"langsmith\";const client = new Client({ apiKey: \"lsv2_...\" });const prompts = client.listPrompts();for await (const prompt of prompts) {console.log(prompt);}// List my private prompts that include \"joke\"const private_joke_prompts = client.listPrompts({ query: \"joke\", isPublic: false});// Delete a promptclient.deletePrompt(\"joke-generator\");// Like a promptclient.likePrompt(\"efriis/my-first-prompt\");// Unlike a promptclient.unlikePrompt(\"efriis/my-first-prompt\");\\nImportant Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.\\nHowever, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain\\'s load method.All other methods in the LangSmith SDK can be used directly.Was this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextManaging Prompt SettingsInstall packagesConfigure environment variablesPush a promptPull a promptUse a prompt without LangChainOpenAIAnthropicList, delete, and like promptsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_docs=loader.load()\n",
    "page_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With different LLM models, there are limited context size, so we cannot give the entire data directly. \\\n",
    "Hence, we need to built chunks of texts from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data -> Docs -> Divide our documents into chunks -> vectors using Vector Embeddings -> Vector Store DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "\n",
    "docs=splitter.split_documents(page_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='Manage prompts programmatically | 🦜️🛠️ LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptUse custom TLS certificatesManage prompts programmaticallyManaging Prompt SettingsPrompt TagsOpen a prompt from a traceTesting over a datasetLangChain HubPrompt CanvasConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringHow-to GuidesManage prompts programmaticallyOn this pageManage prompts programmatically'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.\\nnotePreviously this functionality lived in the langchainhub package which is now deprecated.\\nAll functionality going forward will live in the langsmith package.\\nInstall packages\\u200b\\nIn Python, you can directly use the LangSmith SDK (recommended, full functionality) or you can use through the LangChain package (limited to pushing and pulling prompts).\\nIn TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.\\nPythonLangChain (Python)TypeScriptpip install -U langsmith # version >= 0.1.99pip install -U langchain langsmith # langsmith version >= 0.1.99 and langchain >= 0.2.13yarn add langsmith langchain // langsmith version >= 0.1.99 and langchain version >= 0.2.14\\nConfigure environment variables\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='Configure environment variables\\u200b\\nIf you already have LANGSMITH_API_KEY set to your current workspace\\'s api key from LangSmith, you can skip this step.\\nOtherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key in LangSmith.\\nSet your environment variable.\\nexport LANGSMITH_API_KEY=\"lsv2_...\"\\nTerminologyWhat we refer to as \"prompts\" used to be called \"repos\", so any references to \"repo\" in the code are referring to a prompt.\\nPush a prompt\\u200b\\nTo create a new prompt or update an existing prompt, you can use the push prompt method.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='Push a prompt\\u200b\\nTo create a new prompt or update an existing prompt, you can use the push prompt method.\\nPythonLangChain (Python)TypeScriptfrom langsmith import Clientfrom langchain_core.prompts import ChatPromptTemplateclient = Client()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = client.push_prompt(\"joke-generator\", object=prompt)# url is a link to the prompt in the UIprint(url)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")url = prompts.push(\"joke-generator\", prompt)# url is a link to the prompt in the UIprint(url)import * as hub from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const url = hub.push(\"joke-generator\", {object: prompt,});// url is a link to the prompt in the UIconsole.log(url);'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='You can also push a prompt as a RunnableSequence of a prompt and a model.\\nThis is useful for storing the model configuration you want to use with this prompt.\\nThe provider must be supported by the LangSmith playground. (see settings here: Supported Providers)'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='PythonLangChain (Python)TypeScriptfrom langsmith import Clientfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIclient = Client()model = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelclient.push_prompt(\"joke-generator-with-model\", object=chain)from langchain import hub as promptsfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelurl = prompts.push(\"joke-generator-with-model\", chain)# url is a link to the prompt in the UIprint(url)import * as hub from \"langchain/hub\";import { ChatPromptTemplate } from \"@langchain/core/prompts\";import { ChatOpenAI } from \"@langchain/openai\";const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='\"@langchain/core/prompts\";import { ChatOpenAI } from \"@langchain/openai\";const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");const chain = prompt.pipe(model);await hub.push(\"joke-generator-with-model\", {object: chain});'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content=\"Pull a prompt\\u200b\\nTo pull a prompt, you can use the pull prompt method, which returns a the prompt as a langchain PromptTemplate.\\nTo pull a private prompt you do not need to specify the owner handle (though you can, if you have one set).\\nTo pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt\\'s author.\\nPythonLangChain (Python)TypeScriptfrom langsmith import Clientfrom langchain_openai import ChatOpenAIclient = Client()prompt = client.pull_prompt(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})from langchain import hub as promptsfrom langchain_openai import ChatOpenAIprompt = prompts.pull(\"joke-generator\")model = ChatOpenAI(model=\"gpt-4o-mini\")chain = prompt | modelchain.invoke({\"topic\": \"cats\"})import * as hub from \"langchain/hub\";import { ChatOpenAI } from \"@langchain/openai\";const prompt = await hub.pull(\"joke-generator\");const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });const chain = prompt.pipe(model);await chain.invoke({\"topic\": \"cats\"});\\nSimilar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.\\nJust specify include_model when pulling the prompt.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model.\\nJust specify include_model when pulling the prompt.\\nIf the stored prompt includes a model, it will be returned as a RunnableSequence.\\nMake sure you have the proper environment variables set for the model you are using.\\nPythonLangChain (Python)TypeScriptfrom langsmith import Clientclient = Client()chain = client.pull_prompt(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})from langchain import hub as promptschain = prompts.pull(\"joke-generator-with-model\", include_model=True)chain.invoke({\"topic\": \"cats\"})import * as hub from \"langchain/hub\";import { Runnable } from \"@langchain/core/runnables\";const chain = await hub.pull<Runnable>(\"joke-generator-with-model\", { includeModel: true });await chain.invoke({\"topic\": \"cats\"});\\nWhen pulling a prompt, you can also specify a specific commit hash or prompt tag to pull a specific version of the prompt.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='When pulling a prompt, you can also specify a specific commit hash or prompt tag to pull a specific version of the prompt.\\nPythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"joke-generator:12344e88\")prompt = prompts.pull(\"joke-generator:12344e88\")const prompt = await hub.pull(\"joke-generator:12344e88\")\\nTo pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt\\'s author.\\nPythonLangChain (Python)TypeScriptprompt = client.pull_prompt(\"efriis/my-first-prompt\")prompt = prompts.pull(\"efriis/my-first-prompt\")const prompt = await hub.pull(\"efriis/my-first-prompt\")\\nUse a prompt without LangChain\\u200b\\nIf you want to store your prompts in LangSmith but use them directly with a model provider\\'s API, you can use our conversion methods.\\nThese convert your prompt into the payload required for the OpenAI or Anthropic API.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='These convert your prompt into the payload required for the OpenAI or Anthropic API.\\nThese conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency\\nin addition to your official SDK of choice. Here are some examples:\\nOpenAI\\u200b\\nPythonTypeScriptpip install -U langchain_openaiyarn add @langchain/openai @langchain/core // @langchain/openai version >= 0.3.2'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith.client import Client, convert_prompt_to_openai_format# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai_format(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)import * as hub from \"langchain/hub\";import { convertPromptToOpenAI } from \"@langchain/openai\";import OpenAI from \"openai\";const prompt = await hub.pull(\"jacob/joke-generator\");const formattedPrompt = await prompt.invoke({topic: \"cats\",});const { messages } = convertPromptToOpenAI(formattedPrompt);const openAIClient = new OpenAI();const openAIResponse = await openAIClient.chat.completions.create({model: \"gpt-4o-mini\",messages,});\\nAnthropic\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='Anthropic\\u200b\\nPythonTypeScriptpip install -U langchain_anthropicyarn add @langchain/anthropic @langchain/core // @langchain/anthropic version >= 0.3.3'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='PythonTypeScriptfrom anthropic import Anthropicfrom langsmith.client import Client, convert_prompt_to_anthropic_format# langsmith clientclient = Client()# anthropic clientanthropic_client = Anthropic()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})anthropic_payload = convert_prompt_to_anthropic_format(prompt_value)anthropic_response = anthropic_client.messages.create(**anthropic_payload)import * as hub from \"langchain/hub\";import { convertPromptToAnthropic } from \"@langchain/anthropic\";import Anthropic from \"@anthropic-ai/sdk\";const prompt = await hub.pull(\"jacob/joke-generator\");const formattedPrompt = await prompt.invoke({topic: \"cats\",});const { messages, system } = convertPromptToAnthropic(formattedPrompt);const anthropicClient = new Anthropic();const anthropicResponse = await anthropicClient.messages.create({model: \"claude-3-haiku-20240307\",system,messages,max_tokens: 1024,stream:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='anthropicClient = new Anthropic();const anthropicResponse = await anthropicClient.messages.create({model: \"claude-3-haiku-20240307\",system,messages,max_tokens: 1024,stream: false,});'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='List, delete, and like prompts\\u200b\\nYou can also list, delete, and like/unlike prompts using the list prompts, delete prompt, like prompt and unlike prompt methods.\\nSee the LangSmith SDK client for extensive documentation on these methods.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='See the LangSmith SDK client for extensive documentation on these methods.\\nPythonTypeScript# List all prompts in my workspaceprompts = client.list_prompts()# List my private prompts that include \"joke\"prompts = client.list_prompts(query=\"joke\", is_public=False)# Delete a promptclient.delete_prompt(\"joke-generator\")# Like a promptclient.like_prompt(\"efriis/my-first-prompt\")# Unlike a promptclient.unlike_prompt(\"efriis/my-first-prompt\")// List all prompts in my workspaceimport Client from \"langsmith\";const client = new Client({ apiKey: \"lsv2_...\" });const prompts = client.listPrompts();for await (const prompt of prompts) {console.log(prompt);}// List my private prompts that include \"joke\"const private_joke_prompts = client.listPrompts({ query: \"joke\", isPublic: false});// Delete a promptclient.deletePrompt(\"joke-generator\");// Like a promptclient.likePrompt(\"efriis/my-first-prompt\");// Unlike a promptclient.unlikePrompt(\"efriis/my-first-prompt\");'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content=\"Important Note for JavaScript UsersFor pulling prompts, we recommend using the langchain/hub package, as it handles prompt deserialization automatically.\\nHowever, you can also choose to use the _pullPrompt method of the langsmith package directly but, you will need to manually deserialize the prompt using LangChain's load method.All other methods in the LangSmith SDK can be used directly.Was this page helpful?You can leave detailed feedback on GitHub.PreviousUse custom TLS certificatesNextManaging Prompt SettingsInstall packagesConfigure environment variablesPush a promptPull a promptUse a prompt without LangChainOpenAIAnthropicList, delete, and like promptsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using OpenAIEmbedding model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using FAISS for storing vectorstore DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore_db=FAISS.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x18255375c30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query from Vectorstore DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These convert your prompt into the payload required for the OpenAI or Anthropic API.\\nThese conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency\\nin addition to your official SDK of choice. Here are some examples:\\nOpenAI\\u200b\\nPythonTypeScriptpip install -U langchain_openaiyarn add @langchain/openai @langchain/core // @langchain/openai version >= 0.3.2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"payload required for the OpenAI or Anthropic\"\n",
    "res=vectorstore_db.similarity_search(query)\n",
    "res[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Chain, Documents Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001824E7D8F70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001824E7DB040>, root_client=<openai.OpenAI object at 0x0000018255377CA0>, root_async_client=<openai.AsyncOpenAI object at 0x000001824E7D8FD0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of the conversion methods mentioned in the context?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "  \"input\":\"payload required for the OpenAI or Anthropic\",\n",
    "  \"context\":[Document(page_content=\"Use a prompt without LangChain. If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API. These conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Retriever** \\\n",
    "Input--->Retriever--->vectorstore_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x18255375c30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000018255375C30>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001824E7D8F70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001824E7DB040>, root_client=<openai.OpenAI object at 0x0000018255377CA0>, root_async_client=<openai.AsyncOpenAI object at 0x000001824E7D8FD0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is required to use the conversion methods mentioned in the context for the OpenAI or Anthropic API?\\n\\nTo use the conversion methods for the OpenAI or Anthropic API, you need to install the appropriate LangChain integration packages as a dependency, in addition to your official SDK of choice.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=retrieval_chain.invoke({\n",
    "  \"input\":\"payload required for the OpenAI or Anthropic\"\n",
    "})\n",
    "res['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'payload required for the OpenAI or Anthropic',\n",
       " 'context': [Document(id='35cd6ac2-55e4-4fef-afff-1c76bc1b9556', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='These convert your prompt into the payload required for the OpenAI or Anthropic API.\\nThese conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency\\nin addition to your official SDK of choice. Here are some examples:\\nOpenAI\\u200b\\nPythonTypeScriptpip install -U langchain_openaiyarn add @langchain/openai @langchain/core // @langchain/openai version >= 0.3.2'),\n",
       "  Document(id='189b6a70-6aed-49ec-955d-e70a1e8bf51d', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='anthropicClient = new Anthropic();const anthropicResponse = await anthropicClient.messages.create({model: \"claude-3-haiku-20240307\",system,messages,max_tokens: 1024,stream: false,});'),\n",
       "  Document(id='36096a10-f139-4898-b658-5fb471a16d98', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='PythonTypeScriptfrom anthropic import Anthropicfrom langsmith.client import Client, convert_prompt_to_anthropic_format# langsmith clientclient = Client()# anthropic clientanthropic_client = Anthropic()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})anthropic_payload = convert_prompt_to_anthropic_format(prompt_value)anthropic_response = anthropic_client.messages.create(**anthropic_payload)import * as hub from \"langchain/hub\";import { convertPromptToAnthropic } from \"@langchain/anthropic\";import Anthropic from \"@anthropic-ai/sdk\";const prompt = await hub.pull(\"jacob/joke-generator\");const formattedPrompt = await prompt.invoke({topic: \"cats\",});const { messages, system } = convertPromptToAnthropic(formattedPrompt);const anthropicClient = new Anthropic();const anthropicResponse = await anthropicClient.messages.create({model: \"claude-3-haiku-20240307\",system,messages,max_tokens: 1024,stream:'),\n",
       "  Document(id='7f9ea456-5be9-4760-9711-406c89e3b1d8', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith.client import Client, convert_prompt_to_openai_format# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai_format(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)import * as hub from \"langchain/hub\";import { convertPromptToOpenAI } from \"@langchain/openai\";import OpenAI from \"openai\";const prompt = await hub.pull(\"jacob/joke-generator\");const formattedPrompt = await prompt.invoke({topic: \"cats\",});const { messages } = convertPromptToOpenAI(formattedPrompt);const openAIClient = new OpenAI();const openAIResponse = await openAIClient.chat.completions.create({model: \"gpt-4o-mini\",messages,});\\nAnthropic\\u200b')],\n",
       " 'answer': 'What is required to use the conversion methods mentioned in the context for the OpenAI or Anthropic API?\\n\\nTo use the conversion methods for the OpenAI or Anthropic API, you need to install the appropriate LangChain integration packages as a dependency, in addition to your official SDK of choice.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='35cd6ac2-55e4-4fef-afff-1c76bc1b9556', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='These convert your prompt into the payload required for the OpenAI or Anthropic API.\\nThese conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency\\nin addition to your official SDK of choice. Here are some examples:\\nOpenAI\\u200b\\nPythonTypeScriptpip install -U langchain_openaiyarn add @langchain/openai @langchain/core // @langchain/openai version >= 0.3.2'),\n",
       " Document(id='189b6a70-6aed-49ec-955d-e70a1e8bf51d', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='anthropicClient = new Anthropic();const anthropicResponse = await anthropicClient.messages.create({model: \"claude-3-haiku-20240307\",system,messages,max_tokens: 1024,stream: false,});'),\n",
       " Document(id='36096a10-f139-4898-b658-5fb471a16d98', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='PythonTypeScriptfrom anthropic import Anthropicfrom langsmith.client import Client, convert_prompt_to_anthropic_format# langsmith clientclient = Client()# anthropic clientanthropic_client = Anthropic()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})anthropic_payload = convert_prompt_to_anthropic_format(prompt_value)anthropic_response = anthropic_client.messages.create(**anthropic_payload)import * as hub from \"langchain/hub\";import { convertPromptToAnthropic } from \"@langchain/anthropic\";import Anthropic from \"@anthropic-ai/sdk\";const prompt = await hub.pull(\"jacob/joke-generator\");const formattedPrompt = await prompt.invoke({topic: \"cats\",});const { messages, system } = convertPromptToAnthropic(formattedPrompt);const anthropicClient = new Anthropic();const anthropicResponse = await anthropicClient.messages.create({model: \"claude-3-haiku-20240307\",system,messages,max_tokens: 1024,stream:'),\n",
       " Document(id='7f9ea456-5be9-4760-9711-406c89e3b1d8', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically', 'title': 'Manage prompts programmatically | 🦜️🛠️ LangSmith', 'description': 'You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith.client import Client, convert_prompt_to_openai_format# langsmith clientclient = Client()# openai clientoai_client = OpenAI()# pull prompt and invoke to populate the variablesprompt = client.pull_prompt(\"joke-generator\")prompt_value = prompt.invoke({\"topic\": \"cats\"})openai_payload = convert_prompt_to_openai_format(prompt_value)openai_response = oai_client.chat.completions.create(**openai_payload)import * as hub from \"langchain/hub\";import { convertPromptToOpenAI } from \"@langchain/openai\";import OpenAI from \"openai\";const prompt = await hub.pull(\"jacob/joke-generator\");const formattedPrompt = await prompt.invoke({topic: \"cats\",});const { messages } = convertPromptToOpenAI(formattedPrompt);const openAIClient = new OpenAI();const openAIResponse = await openAIClient.chat.completions.create({model: \"gpt-4o-mini\",messages,});\\nAnthropic\\u200b')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
